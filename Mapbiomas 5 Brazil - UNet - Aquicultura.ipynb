{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mapbiomas 5 Brazil - UNet - Aquicultura.ipynb","provenance":[{"file_id":"1kHTiJJ64Isz0HrrWqx753qqkoEtHWh7g","timestamp":1575994667974},{"file_id":"1iSlHGHEqM8SYmmfKEmTRHjRSeFLY8JiR","timestamp":1572286949862},{"file_id":"18SssHlk76HYQIZ70GcenlGbXxhDoDbJO","timestamp":1570023906985},{"file_id":"1Ptd7eZxzjnxltUfmtMUlwHq9SkJw2lb7","timestamp":1569503830653},{"file_id":"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb","timestamp":1568921435468},{"file_id":"1pDtzqslcr-Q6HCTTb1dcDEC3V_VG3DTY","timestamp":1560383392239},{"file_id":"125c5IN0exNo3qf7m5Fu-32WT1btu-NSG","timestamp":1558560428792}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_TnxhsQO1YP7","colab_type":"text"},"source":["#**Mapbiomas UNET Brazil**\n","This script is proposed to automate the entire mapbiomes deep learning flow\n"]},{"cell_type":"code","metadata":{"id":"neIa46CpciXq","colab_type":"code","colab":{}},"source":["from google.colab import auth\n","auth.authenticate_user()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4D6ArFWrckmS","colab_type":"code","colab":{}},"source":["# Necessary API's install to notebook VM.\n","!pip install earthengine-api --upgrade\n","!pip install oauth2client \n","!pip install tensorboardcolab\n","!pip install google-api-python-client \n","!pip install pygeoj\n","import logging\n","import os\n","logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVqEwqEmXJi0","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jat01FEoUMqg","colab_type":"code","colab":{}},"source":["# Import, authenticate and initialize the Earth Engine library.\n","import ee\n","ee.Authenticate()\n","ee.Initialize()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RnZzcYhcpsQ","colab_type":"code","colab":{}},"source":["# Tensorflow setup.\n","import os\n","%tensorflow_version 1.x\n","%load_ext tensorboard\n","import tensorflow as tf\n","\n","tf.enable_eager_execution()\n","print(tf.__version__)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1hFdpBQfyhN","colab_type":"code","colab":{}},"source":["# Folium setup.\n","import folium\n","print(folium.__version__)\n","\n","# Define the URL format used for Earth Engine generated map tiles.\n","EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60kL3zUiRN6z","colab":{}},"source":["import tensorflow as tf\n","# Specify names locations for outputs in Cloud Storage. \n","# INSERT YOUR GCP BUCKET HERE:\n","VERSION = '2';\n","BUCKET = 'aquaculture_brazil'\n","FOLDER = 'unet-aquicultura-brazil_version2_2018'\n","TRAINING_BASE = 'training_patches_2018'\n","EVAL_BASE = 'eval_patches_2018'\n","MODEL_DIR = '/content/drive/My Drive/BRAZIL_UNETAquicultura/training_'+ VERSION +'/'\n","\n","# Patch Exportation Configs\n","BUCKET_patch = 'mosaic-gee'\n","FOLDER_patch = 'allPatch'\n","FOLDER_classification = 'aquaculture'\n","\n","# Specify inputs (Landsat bands) to the model and the response variable.\n","opticalBands = ['swir1', 'nir', 'red','NDVI','NDWI','NDSI']\n","#thermalBands = ['B10', 'B11']\n","BANDS = opticalBands #+ thermalBands\n","RESPONSE = 'cluster'\n","FEATURES = BANDS + [RESPONSE]\n","\n","# Specify the size and shape of patches expected by the model.\n","KERNEL_SIZE = 256\n","KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n","COLUMNS = [\n","  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n","]\n","FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n","\n","# Sizes of the training and evaluation datasets.\n","TRAIN_SIZE = 16000\n","EVAL_SIZE = 8000\n","\n","# Specify model training parameters.\n","BATCH_SIZE = 16\n","EPOCHS = 10\n","BUFFER_SIZE = 2000\n","OPTIMIZER = 'SGD' \n","LOSS = 'BinaryCrossentropy'\n","METRICS = ['RootMeanSquaredError','Hinge']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgoDc7Hilfc4","colab_type":"text"},"source":["# Imagery\n","\n","Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."]},{"cell_type":"code","metadata":{"id":"-IlgXu-vcUEY","colab_type":"code","colab":{}},"source":["# The image input data is a cloud-masked median composite.\n","mapbiomas4 = ee.Image('projects/samm/Mapbiomas5/Aquicultura/Clusterized/2018');\n","aquicultura = mapbiomas4.eq(1).unmask(0);\n","image = ee.Image('projects/samm/SAMM/Mosaic/2018').addBands(aquicultura)\n","#image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA/Mosaic_ZC_2018_colecao_3_ZC')\n","mapid = image.getMapId({'bands': ['swir1', 'nir', 'red'], 'min': 30, 'max': 150})\n","map = folium.Map(location=[-5.11, -36.65])\n","folium.TileLayer(\n","    tiles=mapid['tile_fetcher'].url_format,\n","    attr='Planet',\n","    overlay=True,\n","    name='Mosaic composite',\n","  ).add_to(map)\n","map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHznnctkJsZJ","colab_type":"text"},"source":["Prepare the response (what we want to predict).  This is impervious surface area (in fraction of a pixel) from the 2016 NLCD dataset.  Display to check."]},{"cell_type":"code","metadata":{"id":"e0wHDyxVirec","colab_type":"code","colab":{}},"source":["mapid = image.select(RESPONSE).getMapId({'min': 0, 'max': 1})\n","map = folium.Map(location=[-5.11, -36.65])\n","folium.TileLayer(\n","    tiles=mapid['tile_fetcher'].url_format,\n","    attr='Google Earth Engine',\n","    overlay=True,\n","    name='nlcd impervious',\n","  ).add_to(map)\n","map.add_child(folium.LayerControl())\n","map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CTS7_ZzPDhhg","colab_type":"text"},"source":["Stack the 2D images (Landsat composite and NLCD impervious surface) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."]},{"cell_type":"code","metadata":{"id":"eGHYsdAOipa4","colab_type":"code","colab":{}},"source":["featureStack = ee.Image.cat([\n","  image.select(BANDS).unmask(0),\n","  image.select(RESPONSE).unmask(0)\n","]).float()\n","\n","list = ee.List.repeat(1, KERNEL_SIZE)\n","lists = ee.List.repeat(list, KERNEL_SIZE)\n","kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n","\n","arrays = featureStack.neighborhoodToArray(kernel)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4djSxBRG2el","colab_type":"text"},"source":["Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."]},{"cell_type":"markdown","metadata":{"id":"ZV890gPHeZqz","colab_type":"text"},"source":["# Sampling\n","\n","The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."]},{"cell_type":"code","metadata":{"id":"ure_WaD0itQY","colab_type":"code","colab":{}},"source":["trainingPolys = ee.FeatureCollection('projects/samm/Mapbiomas5/Aquicultura/Geometries/train_2018')\n","evalPolys = ee.FeatureCollection('projects/samm/Mapbiomas5/Aquicultura/Geometries/test_2018')\n","print(trainingPolys.size().getInfo())\n","\n","polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n","polyImage = polyImage.updateMask(polyImage)\n","\n","mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n","map = folium.Map(location=[-1.3621, -45.2738], zoom_start=5)\n","folium.TileLayer(\n","    tiles=mapid['tile_fetcher'].url_format,\n","    attr='Google Earth Engine',\n","    overlay=True,\n","    name='training polygons',\n","  ).add_to(map)\n","map.add_child(folium.LayerControl())\n","map"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyRpvwENxE-A","colab_type":"code","cellView":"both","colab":{}},"source":["# Convert the feature collections to lists for iteration.\n","trainingPolysList = trainingPolys.toList(trainingPolys.size())\n","evalPolysList = evalPolys.toList(evalPolys.size())\n","# These numbers determined experimentally.\n","n = 50 # Number of shards in each polygon.\n","N = 2000 # Total sample size in each polygon.\n","\n","#Add some generalism\n","TRAIN_SIZE = trainingPolys.size().getInfo()*N\n","EVAL_SIZE = evalPolys.size().getInfo()*N\n","print('TRAIN:'+str(TRAIN_SIZE))\n","print('EVAL:'+str(EVAL_SIZE))\n","\n","# Export all the training data (in many pieces), with one task \n","# per geometry.\n","for g in range(trainingPolys.size().getInfo()):\n","  geomSample = ee.FeatureCollection([])\n","  for i in range(n):\n","    sample = arrays.sample(\n","      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n","      scale = 30, \n","      numPixels = N / n, # Size of the shard.\n","      seed = i,\n","      tileScale = 8\n","    )\n","    geomSample = geomSample.merge(sample)\n","  \n","  desc = TRAINING_BASE + '_g' + str(g)\n","  task = ee.batch.Export.table.toCloudStorage(\n","    collection = geomSample,\n","    description = desc, \n","    bucket = BUCKET, \n","    fileNamePrefix = FOLDER + '/' + desc,\n","    fileFormat = 'TFRecord',\n","    selectors = BANDS + [RESPONSE]\n","  )\n","  task.start()\n","\n","# Export all the evaluation data.\n","for g in range(evalPolys.size().getInfo()):\n","  geomSample = ee.FeatureCollection([])\n","  for i in range(n):\n","    sample = arrays.sample(\n","      region = ee.Feature(evalPolysList.get(g)).geometry(), \n","      scale = 30, \n","      numPixels = N / n,\n","      seed = i,\n","      tileScale = 8\n","    )\n","    geomSample = geomSample.merge(sample)\n","  \n","  desc = EVAL_BASE + '_g' + str(g)\n","  task = ee.batch.Export.table.toCloudStorage(\n","    collection = geomSample,\n","    description = desc, \n","    bucket = BUCKET, \n","    fileNamePrefix = FOLDER + '/' + desc,\n","    fileFormat = 'TFRecord',\n","    selectors = BANDS + [RESPONSE]\n","  )\n","  task.start()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWXrvBE4607G","colab_type":"text"},"source":["# Training data\n","\n","Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."]},{"cell_type":"code","metadata":{"id":"WWZ0UXCVMyJP","colab_type":"code","colab":{}},"source":["def parse_tfrecord(example_proto):\n","  \"\"\"The parsing function.\n","  Read a serialized example into the structure defined by FEATURES_DICT.\n","  Args:\n","    example_proto: a serialized Example.\n","  Returns: \n","    A dictionary of tensors, keyed by feature name.\n","  \"\"\"\n","  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n","\n","\n","def to_tuple(inputs):\n","  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n","  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n","  Args:\n","    inputs: A dictionary of tensors, keyed by feature name.\n","  Returns: \n","    A dtuple of (inputs, outputs).\n","  \"\"\"\n","  inputsList = [inputs.get(key) for key in FEATURES]\n","  stacked = tf.stack(inputsList, axis=0)\n","  # Convert from CHW to HWC\n","  stacked = tf.transpose(stacked, [1, 2, 0])\n","  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n","\n","\n","def get_dataset(pattern):\n","  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n","  Get all the files matching the pattern, parse and convert to tuple.\n","  Args:\n","    pattern: A file pattern to match in a Cloud Storage bucket.\n","  Returns: \n","    A tf.data.Dataset\n","  \"\"\"\n","  glob = tf.gfile.Glob(pattern)\n","  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n","  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n","  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rm0qRF0fAYcC","colab_type":"code","colab":{}},"source":["def get_training_dataset():\n","\t\"\"\"Get the preprocessed training dataset\n","  Returns: \n","    A tf.data.Dataset of training data.\n","  \"\"\"\n","\tglob = 'gs://aquaculture_brazil/unet-aquicultura-brazil_version2_2018/training_patches_2018 + '*'\n","\tprint(glob)\n","\tdataset = get_dataset(glob)\n","\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n","\treturn dataset\n","\n","training = get_training_dataset()\n","\n","print(iter(training.take(1)).next())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-cQO5RL6vob","colab_type":"text"},"source":["# Evaluation data\n","\n","Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."]},{"cell_type":"code","metadata":{"id":"ieKTCGiJ6xzo","colab_type":"code","colab":{}},"source":["def get_eval_dataset():\n","\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + EVAL_BASE + '*'\n","\tdataset = get_dataset(glob)\n","\tdataset = dataset.batch(1).repeat()\n","\treturn dataset\n","\n","evaluation = get_eval_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JIE7Yl87lgU","colab_type":"text"},"source":["# Model\n","\n","Here we use the Keras implementation of the U-Net model as found [in the TensorFlow examples](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb).  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."]},{"cell_type":"code","metadata":{"id":"wsnnnz56yS3l","colab_type":"code","colab":{}},"source":["\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import models\n","from tensorflow.python.keras import metrics\n","from tensorflow.python.keras import optimizers\n","\n","def conv_block(input_tensor, num_filters):\n","\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n","\tencoder = layers.BatchNormalization()(encoder)\n","\tencoder = layers.Activation('relu')(encoder)\n","\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n","\tencoder = layers.BatchNormalization()(encoder)\n","\tencoder = layers.Activation('relu')(encoder)\n","\treturn encoder\n","\n","def encoder_block(input_tensor, num_filters):\n","\tencoder = conv_block(input_tensor, num_filters)\n","\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n","\treturn encoder_pool, encoder\n","\n","def decoder_block(input_tensor, concat_tensor, num_filters):\n","\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n","\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n","\tdecoder = layers.BatchNormalization()(decoder)\n","\tdecoder = layers.Activation('relu')(decoder)\n","\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","\tdecoder = layers.BatchNormalization()(decoder)\n","\tdecoder = layers.Activation('relu')(decoder)\n","\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","\tdecoder = layers.BatchNormalization()(decoder)\n","\tdecoder = layers.Activation('relu')(decoder)\n","\treturn decoder\n","\n","def get_model():\n","\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n","\tencoder0_pool, encoder0 = encoder_block(inputs, 64) # 128\n","\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 128) # 64\n","\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 256) # 32\n","\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 512) # 16\n","\tcenter = conv_block(encoder3_pool, 1024) # center\n","\tdecoder4 = decoder_block(center, encoder3, 512) # 16\n","\tdecoder3 = decoder_block(decoder4, encoder2, 256) # 32\n","\tdecoder2 = decoder_block(decoder3, encoder1, 128) # 64\n","\tdecoder1 = decoder_block(decoder2, encoder0, 64) # 128\n","\t#outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder1)\n","\t#dropout = tf.keras.layers.Dropout(decoder1, rate=0.5, name=\"dropout\")\n","\tdropout = layers.Dropout(0.2, name=\"dropout\", noise_shape=None, seed=None)(decoder1)\n","\toutputs = layers.Conv2D(1, (1, 1),  activation=tf.nn.sigmoid, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())(dropout)\n","\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n","\t#loss = twoclass_cost(output, labels_clipped)\n","\toptimizer = tf.contrib.opt.NadamOptimizer(0.05, name='optimizer')\n","\tmodel.compile(\n","\t\toptimizer=optimizer, \n","\t\tloss=losses.get(LOSS),\n","\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n","\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibbpwTXSaZVc","colab_type":"code","colab":{}},"source":["#Tensorflow Image Prediction sampling\n","import keras\n","from PIL import Image\n","import io\n","import numpy as np\n","import tensorflow as t\n","\n","class ImageHistory(keras.callbacks.Callback):\n","    \n","    def __init__(self, tensor_board_dir, data, last_step=0, draw_interval=100):\n","        self.data = data\n","        self.last_step = last_step\n","        self.draw_interval = draw_interval\n","        self.tensor_board_dir = tensor_board_dir\n","        \n","    def on_train_begin(self, logs={}):\n","        return\n","\n","    def on_train_end(self, logs={}):\n","        return\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        return\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        images = []\n","        labels = []\n","        for item in self.data:\n","            image_data = item[0]\n","            label_data = item[1]\n","            y_pred = self.model.predict(image_data)\n","            images.append(y_pred)\n","            labels.append(label_data)\n","        image_data = np.concatenate(images,axis=2)\n","        label_data = np.concatenate(labels,axis=2)\n","        data = np.concatenate((image_data,label_data), axis=1)\n","        self.saveToTensorBoard(data, 'epoch', epoch)\n","        return\n","\n","    def on_batch_begin(self, batch, logs={}):\n","        return\n","\n","    def on_batch_end(self, batch, logs={}):\n","        if batch % self.draw_interval == 0:\n","            images = []\n","            labels = []\n","            for item in self.data:\n","                image_data = item[0]\n","                label_data = item[1]\n","                y_pred = self.model.predict(image_data)\n","                images.append(y_pred)\n","                labels.append(label_data)\n","            image_data = np.concatenate(images,axis=2)\n","            label_data = np.concatenate(labels,axis=2)\n","            data = np.concatenate((image_data,label_data), axis=1)\n","            self.last_step += 1\n","            self.saveToTensorBoard(data, 'batch', self.last_step*self.draw_interval)\n","        return\n","    \n","    def make_image(self, npyfile):\n","        \"\"\"\n","        Convert an numpy representation image to Image protobuf.\n","        taken and updated from https://github.com/lanpa/tensorboard-pytorch/\n","        \"\"\"\n","        height, width, channel = npyfile.shape\n","        image = Image.frombytes('L',(width,height), npyfile.tobytes())\n","        output = io.BytesIO()\n","        image.save(output, format='PNG')\n","        image_string = output.getvalue()\n","        output.close()\n","        return tf.Summary.Image(height=height, width=width, colorspace=channel,\n","                             encoded_image_string=image_string)\n","    \n","    def saveToTensorBoard(self, npyfile, tag, epoch):\n","        data = npyfile[0,:,:,:]\n","        image = (((data - data.min()) * 255) / (data.max() - data.min())).astype(np.uint8)\n","        image = self.make_image(image)\n","        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, image=image)])\n","        writer = tf.summary.FileWriter(self.tensor_board_dir)\n","        writer.add_summary(summary, epoch)\n","        writer.close()       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uu_E7OTDBCoS","colab_type":"text"},"source":["# Training the model\n","\n","You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."]},{"cell_type":"code","metadata":{"id":"Thjr6jXeNkc_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","m = get_model()\n","#print(m.summary())\n","\n","MODEL_DIR = '/content/drive/My Drive/BRAZIL_UNETAquicultura/training_2/cp-0074.ckpt' #2018+2005\n","m.load_weights(MODEL_DIR)\n","\n","#loss, acc = m.evaluate(test_images, test_labels)\n","#print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ1-zS8HaKLa","colab_type":"code","colab":{}},"source":["import keras.backend as K\n","\n","%tensorboard --logdir .logs\n","# define TensorBoard directory\n","tb_dir = '/content/logs/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzzaWxOhSxBy","colab_type":"code","colab":{}},"source":["checkpoint_path = \"/content/drive/My Drive/BRAZIL_UNETAquicultura/training_\" + VERSION + \"/cp-{epoch:04d}.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    checkpoint_path, verbose=1, save_weights_only=True, period=2)\n","m.save_weights(checkpoint_path.format(epoch=0))\n","\n","image_history = ImageHistory(tensor_board_dir=tb_dir,\n","        data=training, last_step=0, draw_interval=100)\n","\n","#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(\"/content/logs\", histogram_freq=1)\n","\n","with tf.device('/device:GPU:0'):\n","  m.fit(\n","      x=training, \n","      epochs=80, \n","      initial_epoch=74, # REMEBER TO CHANGE THIS INITIAL EPOCH PARAM, WHEN ANOTHER MODEL HAS BEEN LOADED\n","      steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n","      validation_data=evaluation,\n","      validation_steps=EVAL_SIZE,\n","      callbacks = [cp_callback,tensorboard_callback])#image_history])\n","      #callbacks = [cp_callback,TensorBoardColabCallback(tbc)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RJpNfEUS1qp","colab_type":"code","colab":{}},"source":["# Load a trained model. 50 epochs. 25 hours. Final RMSE ~0.08.\n","#MODEL_DIR = '/content/drive/My Drive/FCNN/training/cp.ckpt'\n","#m = tf.contrib.saved_model.load_keras_model(MODEL_DIR)\n","#m.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3WDAa-RUpXP","colab_type":"code","colab":{}},"source":["def doExport(out_image_base,index_in, kernel_buffer, bjregion):\n","  \"\"\"Run the image export task.  Block until complete.\n","  \"\"\"\n","  index = index_in\n","  image = ee.Image('projects/samm/SAMM/Mosaic/'+str(index))\n","  #image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA/Mosaic_ZC_'+str(index)+'_colecao_3_ZC')\n","  out_image_base2 = out_image_base+'_'+str(index_in)\n","  filesList = !gsutil ls 'gs://'{BUCKET_patch}'/'{FOLDER_patch}'/'{out_image_base2}'*'\n","  exportFilesList = [s for s in filesList if out_image_base in s]\n","  if(len(exportFilesList) > 1):\n","    print('Image Already Exported')\n","    return None\n","  print(\"Exporting..\")\n","  \n","  task = ee.batch.Export.image.toCloudStorage(\n","    image = image.select(BANDS).toFloat(), \n","    description = out_image_base+'_'+str(index), \n","    bucket = BUCKET_patch, \n","    fileNamePrefix = FOLDER_patch + '/' + out_image_base+'_'+str(index), \n","    region = bjregion, \n","    scale = 30, \n","    fileFormat = 'TFRecord', \n","    maxPixels = 1e13,\n","    formatOptions = { \n","      'patchDimensions': KERNEL_SHAPE,\n","      'kernelSize': kernel_buffer,\n","      'compressed': True,\n","      'maxFileSize': 104857600\n","    }\n","  )\n","  task.start()\n","\n","  # Block until the task completes.\n","  print('Running image export to Cloud Storage...')\n","  import time\n","  #while task.active():\n","  time.sleep(5)\n","\n","  # Error condition\n","  if task.status()['state'] != 'COMPLETED':\n","    print('Error with image export.')\n","  else:\n","    print('Image export completed.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zb_9_FflygVw","colab_type":"code","colab":{}},"source":["def doPrediction(out_image_base,index_in, user_folder, kernel_buffer, region):\n","  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n","  \"\"\"\n","  #BUGFIX YEAR \n","  out_image_base = out_image_base+'_'+str(index_in)\n","  #check if this file already exists on bucket\n","  # Get a list of all the files in the output bucket.\n","  filesList = !gsutil ls 'gs://'{BUCKET_patch}'/'{FOLDER_patch}'/'{out_image_base}'*'\n","  # Get only the files generated by the image export.\n","  exportFilesList = [s for s in filesList if out_image_base in s]\n","\n","  # Get the list of image files and the JSON mixer file.\n","  imageFilesList = []\n","  jsonFile = None\n","  for f in exportFilesList:\n","    if f.endswith('.tfrecord.gz'):\n","      imageFilesList.append(f)\n","    elif f.endswith('.json'):\n","      jsonFile = f\n","\n","  # Make sure the files are in the right order.\n","  imageFilesList.sort()\n","  \n","  \n","  predictioned_file =  !gsutil ls 'gs://'{BUCKET_patch}'/'{FOLDER_classification}'/'{out_image_base}'.TFRecord'\n","  out_image_asset = user_folder + '/' + out_image_base\n","  out_image_file = 'gs://' + BUCKET_patch + '/' + FOLDER_classification + '/' + out_image_base + '.TFRecord'\n","\n","  if predictioned_file[0] == 'gs://'+ BUCKET_patch+'/'+FOLDER_classification+'/'+out_image_base+'.TFRecord':\n","    print('classification was already on Bucket')\n","    try:\n","      gee_asset = ee.Image(out_image_asset).bandNames().getInfo()\n","      if len(gee_asset) > 0:\n","        print('classification was already on GEE')\n","        return None \n","    except ee.EEException:\n","      print('classification has not been uploaded')\n","      print('!earthengine --no-use_cloud_api upload image --asset_id='+out_image_asset+' '+out_image_file+' '+jsonFile)\n","      !earthengine --no-use_cloud_api upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n","    return None \n","    \n","  import json\n","  # Load the contents of the mixer file to a JSON object.\n","  jsonText = !gsutil cat {jsonFile}\n","  # Get a single string w/ newlines from the IPython.utils.text.SList\n","  mixer = json.loads(jsonText.nlstr)\n","  #pprint(mixer)\n","  patches = mixer['totalPatches']\n","  \n","  # Get set up for prediction.\n","  x_buffer = int(kernel_buffer[0] / 2)\n","  y_buffer = int(kernel_buffer[1] / 2)\n","\n","  buffered_shape = [\n","      KERNEL_SHAPE[0] + kernel_buffer[0],\n","      KERNEL_SHAPE[1] + kernel_buffer[1]]\n","\n","  imageColumns = [\n","    tf.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n","      for k in BANDS\n","  ]\n","\n","  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n","\n","  def parse_image(example_proto):\n","    return tf.parse_single_example(example_proto, imageFeaturesDict)\n","\n","  def toTupleImage(dict):\n","    inputsList = [dict.get(key) for key in BANDS]\n","    stacked = tf.stack(inputsList, axis=0)\n","    stacked = tf.transpose(stacked, [1, 2, 0])\n","    return stacked\n","  \n","  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n","  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n","  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n","  imageDataset = imageDataset.map(toTupleImage).batch(1)\n","  \n","  # Perform inference.\n","  print('Running predictions...')\n","  with tf.device('/device:GPU:0'):\n","    predictions = m.predict(imageDataset, steps=patches, verbose=1)\n","  # print(predictions[0])\n","\n","  #print('Writing predictions...')\n","  out_image_file = 'gs://' + BUCKET_patch + '/' + FOLDER_classification + '/' + out_image_base + '.TFRecord'\n","  writer = tf.python_io.TFRecordWriter(out_image_file)\n","  patches = 0\n","  for predictionPatch in predictions:\n","    #print('Writing patch ' + str(patches) + '...')\n","    predictionPatch = predictionPatch[\n","        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n","\n","    # Create an example.\n","    example = tf.train.Example(\n","      features=tf.train.Features(\n","        feature={\n","          'impervious': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=predictionPatch.flatten()))\n","        }\n","      )\n","    )\n","    # Write the example.\n","    writer.write(example.SerializeToString())\n","    patches += 1\n","\n","  writer.close()\n","\n","  # Start the upload.\n","  out_image_asset = user_folder + '/' + out_image_base\n","  !earthengine --no-use_cloud_api upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZqlymOehnQO","colab_type":"text"},"source":["Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs.  In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel.  "]},{"cell_type":"code","metadata":{"id":"FPANwc7B1-TS","colab_type":"code","colab":{}},"source":["import pygeoj\n","kernel_buffer = [256, 256]\n","image_base_name = 'UNET_grid_'\n","user_folder = 'projects/mapbiomas-workspace/TRANSVERSAIS/AQUICULTURA_5' # INSERT YOUR FOLDER HERE.\n","# Base file name to use for TFRecord files and assets.\n","grid = pygeoj.load(\"/content/drive/My Drive/BRAZIL_UNETAquicultura/Grid_2deg_ZC_ovr.geojson\")\n","print(\"Total Features on GRID\")\n","print(len(grid))\n","print(grid.get_feature(1).geometry.coordinates)\n","\n","bj_region =  None;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLNEOLkXWvSi","colab_type":"code","colab":{}},"source":["# Run the export.\n","for region in grid:\n","    region_id = region.properties['id']\n","    if int(region_id) > 0:\n","      print('Region:')\n","      print(region_id)\n","      for y in range(1985, 2000):\n","          doExport(image_base_name+str(region_id)+str('_' + VERSION),y, kernel_buffer, region.geometry.coordinates)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZQ1f31L-WSz","colab_type":"code","colab":{}},"source":["\n","for  y in range(1991, 2018):\n","    for region in grid:\n","      region_id = region.properties['id']\n","      print(region_id)\n","      if int(region_id) > 0 :\n","        doPrediction(image_base_name+str(int(region_id))+str('_' + VERSION),y, user_folder, kernel_buffer, region.geometry.coordinates)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jgco6HJ4R5p2","colab_type":"code","colab":{}},"source":["imageCollection = ee.ImageCollection('projects/nexgenmap/PSScene4Band')\n","size = 50\n","index = 21\n","imageCollection = imageCollection.filterBounds(bj_region).toList(size)\n","imageInp = ee.Image(imageCollection.get(index))\n","imageArea = imageInp.mask().int().mask().int().reduceToVectors(ee.Reducer.max(), bj_region, 3, None, True, None, None, None, True, 1e13, 1, True).filterMetadata('label','equals',1)\n","mapid2 = imageInp.getMapId({'bands': ['R', 'G', 'B'], 'min': 2000, 'max': 11000})\n","\n","out_image = ee.Image(user_folder + '/' + bj_image_base)\n","mapid = out_image.getMapId({'min': 0, 'max': 1})\n","map = folium.Map(location=[-1.3621, -45.2738])\n","folium.TileLayer(\n","    tiles=EE_TILES.format(**mapid),\n","    attr='Google Earth Engine',\n","    overlay=True,\n","    name='Predict',\n","  ).add_to(map)\n","folium.TileLayer(\n","    tiles=EE_TILES.format(**mapid2),\n","    attr='Google Earth Engine',\n","    overlay=True,\n","    name='Input',\n","  ).add_to(map)\n","map.add_child(folium.LayerControl())\n","map"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8OmlCg5pHXT","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}